{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### 0. Imports"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import tqdm\n",
    "from torchvision.utils import save_image, make_grid\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from gan import Generator, Discriminator"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1. Setup"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "WORKING_ENV = 'PAPERSPACE'\n",
    "assert WORKING_ENV in ['LABS', 'COLAB', 'PAPERSPACE']\n",
    "\n",
    "if WORKING_ENV == 'COLAB':\n",
    "    from google.colab import drive\n",
    "    %load_ext google.colab.data_table\n",
    "    content_path = '/content/drive/MyDrive/vae'\n",
    "    data_path = './data/'\n",
    "    drive.mount('/content/drive/')\n",
    "\n",
    "else:\n",
    "    !pip install ipywidgets\n",
    "    content_path = '/notebooks'\n",
    "    data_path = './data/'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "mean = torch.Tensor([0.5, 0.5, 0.5])\n",
    "std = torch.Tensor([0.5, 0.5, 0.5])\n",
    "unnormalize = transforms.Normalize((-mean / std).tolist(), (1.0 / std).tolist())\n",
    "\n",
    "def denorm(x, channels=None, w=None ,h=None, resize = False):\n",
    "\n",
    "    x = unnormalize(x)\n",
    "    if resize:\n",
    "        if channels is None or w is None or h is None:\n",
    "            print('Number of channels, width and height must be provided for resize.')\n",
    "        x = x.view(x.size(0), channels, w, h)\n",
    "    return x\n",
    "\n",
    "def show(img):\n",
    "    npimg = img.cpu().numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1,2,0)))\n",
    "\n",
    "if not os.path.exists(content_path + '/GAN'):\n",
    "    os.makedirs(content_path + '/GAN')\n",
    "\n",
    "GPU = True\n",
    "if GPU:\n",
    "    device = torch.device(\"cuda\"  if torch.cuda.is_available() else \"cpu\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(f'Using {device}')\n",
    "\n",
    "# We set a random seed to ensure that your results are reproducible.\n",
    "if torch.cuda.is_available():\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "torch.manual_seed(0)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2. Load data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "image_size = 32\n",
    "\n",
    "transform = transforms.Compose([\n",
    "     transforms.ToTensor(),\n",
    "     transforms.Normalize(mean=mean, std=std),\n",
    "])\n",
    "# note - data_path was initialized at the top of the notebook\n",
    "cifar10_train = datasets.CIFAR10(data_path, train=True, download=True, transform=transform)\n",
    "cifar10_test = datasets.CIFAR10(data_path, train=False, download=True, transform=transform)\n",
    "loader_train = DataLoader(cifar10_train, batch_size=batch_size)\n",
    "loader_test = DataLoader(cifar10_test, batch_size=batch_size)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3. Define hyperparameters and initialise models"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "num_epochs = 20\n",
    "learning_rate = 0.0002\n",
    "learning_rate_G = 0.0002\n",
    "learning_rate_D = 0.0001\n",
    "latent_vector_size = 150\n",
    "\n",
    "generator_hyper_params = {\n",
    "    \"latent_vector_size\": latent_vector_size,\n",
    "    \"gen_feature_map\": 150,\n",
    "}\n",
    "\n",
    "discriminator_hyper_params = {\n",
    "    \"disc_feature_map\": 64,\n",
    "}\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)\n",
    "\n",
    "\n",
    "def init_models(generator_hyper_params, disc_hyper_params):\n",
    "    use_weights_init = True\n",
    "\n",
    "    model_G = Generator(generator_hyper_params=generator_hyper_params).to(device)\n",
    "    if use_weights_init:\n",
    "        model_G.apply(weights_init)\n",
    "    params_G = sum(p.numel() for p in model_G.parameters() if p.requires_grad)\n",
    "    print(\"Total number of parameters in Generator is: {}\".format(params_G))\n",
    "    print(model_G)\n",
    "    print('\\n')\n",
    "\n",
    "    model_D = Discriminator(discriminator_hyper_params=disc_hyper_params).to(device)\n",
    "    if use_weights_init:\n",
    "        model_D.apply(weights_init)\n",
    "    params_D = sum(p.numel() for p in model_D.parameters() if p.requires_grad)\n",
    "    print(\n",
    "        \"Total number of parameters in Discriminator is: {}\".format(params_D))\n",
    "    print(model_D)\n",
    "    print('\\n')\n",
    "\n",
    "    print(\"Total number of parameters is: {}\".format(params_G + params_D))\n",
    "    return model_G, model_D, params_G, params_D"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4. Train the model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "***Define the loss and initialise optimisers***\n",
    "NB we are defining a model with historical averaging, hence the 'hist' suffix."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def G_loss_function(y, y_hat):\n",
    "    loss = nn.BCELoss()\n",
    "    return loss(y_hat, y)\n",
    "\n",
    "def D_loss_function(y, y_hat):\n",
    "    loss = nn.BCELoss()\n",
    "    return loss(y_hat, y)\n",
    "\n",
    "def loss_hist(params, params_avg):\n",
    "    loss = F.mse_loss(params, params_avg)\n",
    "    return loss"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_G_hist, model_D_hist, params_G_hist, params_D_hist  = init_models(generator_hyper_params=generator_hyper_params,\n",
    "                                                                        disc_hyper_params=discriminator_hyper_params)\n",
    "beta1 = 0.5\n",
    "optimizerD = torch.optim.Adam(model_D_hist.parameters(), lr=learning_rate_D, betas=(beta1, 0.999))\n",
    "optimizerG = torch.optim.Adam(model_G_hist.parameters(), lr=learning_rate_G, betas=(beta1, 0.999))\n",
    "\n",
    "fixed_noise = torch.randn(batch_size, latent_vector_size, 1, 1, device=device)\n",
    "real_label = 0.9\n",
    "fake_label = 0.1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Logging"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_losses_G_hist = []\n",
    "train_losses_D_hist = []\n",
    "train_losses_G_all = []\n",
    "train_losses_D_all = []"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Training loop"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    param_avg_G = torch.zeros(params_G_hist).to(device)\n",
    "    param_avg_D = torch.zeros(params_D_hist).to(device)\n",
    "    n = 0\n",
    "    with tqdm.tqdm(loader_train, unit=\"batch\") as tepoch:\n",
    "        for i, data in enumerate(tepoch):\n",
    "            train_loss_D = 0\n",
    "            train_loss_G = 0\n",
    "\n",
    "            #######################################################################\n",
    "            #                  ** TRAIN DISCRIMINATOR WITH REAL **\n",
    "            #######################################################################\n",
    "            model_D_hist.zero_grad()\n",
    "            real_cpu = data[0].to(device)\n",
    "            size = real_cpu.size(0)\n",
    "            output_real = model_D_hist(real_cpu).view(-1)\n",
    "            target = torch.full((size,), real_label, dtype=torch.float, device=device)\n",
    "            errD_real = D_loss_function(target, output_real)\n",
    "            errD_real.backward()\n",
    "            D_x = output_real.mean().item()\n",
    "\n",
    "            #######################################################################\n",
    "            #                  ** TRAIN DISCRIMINATOR WITH FAKE **\n",
    "            #######################################################################\n",
    "            noise = torch.randn(size, latent_vector_size, 1, 1, device=device)\n",
    "            fake_image = model_G_hist(noise)\n",
    "            target = target.fill_(fake_label)\n",
    "            output_fake = model_D_hist(fake_image.detach()).view(-1)\n",
    "\n",
    "            #######################################################################\n",
    "            #                          ** UPDATE GRADIENTS **\n",
    "            #######################################################################\n",
    "            errD_fake = D_loss_function(target, output_fake)\n",
    "            errD_fake.backward()\n",
    "\n",
    "            #######################################################################\n",
    "            #                          ** PERFORM HISTORICAL AVERAGING **\n",
    "            #######################################################################\n",
    "            param_D_current = torch.cat([param.view(-1) for param in model_D_hist.parameters()]).to(device)\n",
    "            param_avg_D = ( (n*param_avg_D.detach()  + param_D_current.detach()) / (n+1)).to(device)\n",
    "            errD_hist_avg = loss_hist(param_D_current, param_avg_D)\n",
    "            errD_hist_avg.backward()\n",
    "            errD = errD_real + errD_fake + errD_hist_avg\n",
    "            D_G_z1 = output_fake.mean().item()\n",
    "            train_loss_D += errD.item()\n",
    "            optimizerD.step()\n",
    "\n",
    "            #######################################################################\n",
    "            #                     ** UPDATE GENERATOR NETWORK **\n",
    "            #######################################################################\n",
    "            model_G_hist.zero_grad()\n",
    "            output = model_D_hist(fake_image).view(-1)\n",
    "            target = target.fill_(real_label)\n",
    "            errG_standard = G_loss_function(target, output)\n",
    "            errG_standard.backward()\n",
    "            D_G_z2 = output.mean().item()\n",
    "\n",
    "            #######################################################################\n",
    "            #            ** PERFORM HISTORICAL AVERAGING ON GENERATOR **\n",
    "            #######################################################################\n",
    "            param_G_current = torch.cat([param.view(-1) for param in model_G_hist.parameters()]).to(device)\n",
    "            param_avg_G = ((n * param_avg_G.detach() + param_G_current.detach())/ (n+1)).to(device)\n",
    "            errG_avg = loss_hist(param_G_current, param_avg_G)\n",
    "            errG_avg.backward()\n",
    "            errG = errG_avg + errG_standard\n",
    "            train_loss_G += errG.item()\n",
    "            optimizerG.step()\n",
    "            train_losses_D_all.append(errD.item())\n",
    "            train_losses_G_all.append(errG.item())\n",
    "            n += 1\n",
    "\n",
    "            # Logging\n",
    "            if i % 50 == 0:\n",
    "                tepoch.set_description(f\"Epoch {epoch}\")\n",
    "                tepoch.set_postfix(D_G_z=f\"{D_G_z1:.3f}/{D_G_z2:.3f}\", D_x=D_x,\n",
    "                                  Loss_D=errD.item(), Loss_G=errG.item())\n",
    "\n",
    "    if epoch == 0:\n",
    "        save_image(denorm(real_cpu.cpu()).float(), content_path + '/GAN/historical_averaging/real_samples.png')\n",
    "    with torch.no_grad():\n",
    "        fake = model_G_hist(fixed_noise)\n",
    "        save_image(denorm(fake.cpu()).float(), content_path + '/GAN/historical_averaging/fake_samples_epoch{epoch}.png')\n",
    "    train_losses_D_hist.append(train_loss_D)\n",
    "    train_losses_G_hist.append(train_loss_G)\n",
    "\n",
    "\n",
    "torch.jit.save(torch.jit.trace(model_G_hist, (fixed_noise)), content_path + '/GAN/historical_averaging/GAN_G_model.pth')\n",
    "torch.jit.save(torch.jit.trace(model_D_hist, (fake)), content_path + '/GAN/historical_averaging/GAN_D_model.pth')\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 5. Show generator samples"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "input_noise = torch.randn(100, latent_vector_size, 1, 1, device=device)\n",
    "with torch.no_grad():\n",
    "    generated = model_G_hist(input_noise).cpu()\n",
    "    generated = make_grid(denorm(generated)[:100], nrow=10, padding=2, normalize=False,\n",
    "                        range=None, scale_each=False, pad_value=0)\n",
    "    plt.figure(figsize=(15,15))\n",
    "    save_image(generated, content_path/'CW_GAN/Teaching_final.png')\n",
    "    show(generated)\n",
    "\n",
    "it = iter(loader_test)\n",
    "sample_inputs, _ = next(it)\n",
    "fixed_input = sample_inputs[0:64, :, :, :]\n",
    "\n",
    "img = make_grid(denorm(fixed_input), nrow=8, padding=2, normalize=False,\n",
    "                range=None, scale_each=False, pad_value=0)\n",
    "plt.figure(figsize=(15,15))\n",
    "show(img)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
